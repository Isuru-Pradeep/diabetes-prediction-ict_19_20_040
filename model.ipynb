{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d276d0e7-67e8-434c-814c-d5e9537e2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_8220\\1553483019.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "Created output directory: output_20250802_202031\n",
      "Logging to file: output_20250802_202031\\training_log.log\n",
      "numpy version: 1.26.4\n",
      "pandas version: 2.3.1\n",
      "matplotlib version: 3.10.5\n",
      "seaborn version: 0.13.2\n",
      "scikit-learn version: 1.2.2\n",
      "tensorflow version: 2.15.0\n",
      "Num GPUs Available: 0\n",
      "============================================================\n",
      "DIABETES PREDICTION USING NEURAL NETWORKS\n",
      "============================================================\n",
      "Problem Statement:\n",
      "Diabetes is a chronic condition affecting millions worldwide.\n",
      "Early prediction enables better management and prevents complications.\n",
      "This project builds a neural network to predict diabetes using the Pima Indian Diabetes Dataset.\n",
      "\n",
      "============================================================\n",
      "DATA PREPARATION\n",
      "============================================================\n",
      "Dataset loaded successfully from URL!\n",
      "\n",
      "Dataset Shape: (768, 9)\n",
      "Features: ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
      "\n",
      "Dataset Overview:\n",
      "None\n",
      "\n",
      "Statistical Summary:\n",
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n",
      "\n",
      "Target Variable Distribution:\n",
      "Non-diabetic (0): 500 (65.1%)\n",
      "Diabetic (1): 268 (34.9%)\n",
      "\n",
      "Missing Values:\n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "\n",
      "Zero Values Analysis:\n",
      "Glucose: 5 zeros (0.7%)\n",
      "BloodPressure: 35 zeros (4.6%)\n",
      "SkinThickness: 227 zeros (29.6%)\n",
      "Insulin: 374 zeros (48.7%)\n",
      "BMI: 11 zeros (1.4%)\n",
      "\n",
      "Handling Missing/Zero Values:\n",
      "Replaced Glucose zeros with median: 117.00\n",
      "Replaced BloodPressure zeros with median: 72.00\n",
      "Replaced SkinThickness zeros with median: 29.00\n",
      "Replaced Insulin zeros with median: 125.00\n",
      "Replaced BMI zeros with median: 32.30\n",
      "Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_8220\\1553483019.py:204: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x='Outcome', data=df, palette=['skyblue', 'salmon'])\n",
      "Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Splitting and Scaling:\n",
      "Training set: 537 samples (~70%)\n",
      "Validation set: 115 samples (~15%)\n",
      "Test set: 116 samples (~15%)\n",
      "Training set distribution:\n",
      "Outcome\n",
      "0    0.651769\n",
      "1    0.348231\n",
      "Name: proportion, dtype: float64\n",
      "Validation set distribution:\n",
      "Outcome\n",
      "0    0.652174\n",
      "1    0.347826\n",
      "Name: proportion, dtype: float64\n",
      "Test set distribution:\n",
      "Outcome\n",
      "0    0.646552\n",
      "1    0.353448\n",
      "Name: proportion, dtype: float64\n",
      "Note: Stratified splitting ensures class distribution is maintained across all sets.\n",
      "Feature scaling completed using StandardScaler\n",
      "Scaling ensures features have mean=0 and variance=1, improving model convergence.\n",
      "\n",
      "============================================================\n",
      "MODEL DEVELOPMENT\n",
      "============================================================\n",
      "Training Phase Details:\n",
      "- The model is trained on 70% of the data to learn patterns.\n",
      "- Early stopping monitors validation loss to prevent overfitting.\n",
      "- Learning rate reduction adjusts the learning rate if validation loss plateaus.\n",
      "Suggestions for Training Improvement:\n",
      "- Use k-fold cross-validation to reduce variance in performance estimates.\n",
      "- Implement data augmentation techniques for small datasets.\n",
      "- Use class weights to handle imbalanced data.\n",
      "- Explore batch normalization to stabilize training.\n",
      "\n",
      "Building Initial Model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                144       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161 (644.00 Byte)\n",
      "Trainable params: 161 (644.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 8ms/step - loss: 0.8388 - accuracy: 0.4302 - val_loss: 0.7548 - val_accuracy: 0.5043 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7318 - accuracy: 0.5400 - val_loss: 0.6729 - val_accuracy: 0.6261 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.6518 - val_loss: 0.6190 - val_accuracy: 0.6957 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6277 - accuracy: 0.6704 - val_loss: 0.5835 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5809 - accuracy: 0.6853 - val_loss: 0.5603 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7132 - val_loss: 0.5456 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5363 - accuracy: 0.7263 - val_loss: 0.5355 - val_accuracy: 0.7565 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7281 - val_loss: 0.5313 - val_accuracy: 0.7565 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7412 - val_loss: 0.5261 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7505 - val_loss: 0.5245 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7672 - val_loss: 0.5217 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7449 - val_loss: 0.5215 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7654 - val_loss: 0.5201 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.7561 - val_loss: 0.5239 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7765 - val_loss: 0.5262 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7709 - val_loss: 0.5228 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7598 - val_loss: 0.5250 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4864 - accuracy: 0.7691 - val_loss: 0.5216 - val_accuracy: 0.7478 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7728 - val_loss: 0.5235 - val_accuracy: 0.7478 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7691 - val_loss: 0.5248 - val_accuracy: 0.7478 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7765 - val_loss: 0.5248 - val_accuracy: 0.7478 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7728 - val_loss: 0.5261 - val_accuracy: 0.7391 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7840 - val_loss: 0.5270 - val_accuracy: 0.7391 - lr: 5.0000e-04\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved Initial model results to output_20250802_202031\\initial_model_results.txt\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "Hyperparameter tuning uses a grid search over the configuration parameters.\n",
      "Validation set performance guides the selection of the best model parameters.\n",
      "Suggestions for Tuning Improvement:\n",
      "- Use random search or Bayesian optimization for efficiency.\n",
      "- Expand the parameter grid for finer granularity.\n",
      "- Incorporate learning rate schedules for adaptive learning.\n",
      "Parameter grid for Manual Search:\n",
      "  hidden_layers: [1, 2]\n",
      "  neurons: [8, 16, 32]\n",
      "  dropout_rate: [0.2, 0.3]\n",
      "  learning_rate: [0.01, 0.001]\n",
      "\n",
      "Starting hyperparameter tuning...\n",
      "Testing: layers=1, neurons=8, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=1, neurons=8, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=1, neurons=8, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=1, neurons=8, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.6957\n",
      "Testing: layers=1, neurons=16, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=1, neurons=16, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=1, neurons=16, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7652\n",
      "Testing: layers=1, neurons=16, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.7130\n",
      "Testing: layers=1, neurons=32, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=1, neurons=32, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=1, neurons=32, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=1, neurons=32, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.7739\n",
      "Testing: layers=2, neurons=8, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7391\n",
      "Testing: layers=2, neurons=8, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7652\n",
      "Testing: layers=2, neurons=8, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7217\n",
      "Testing: layers=2, neurons=8, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.6957\n",
      "Testing: layers=2, neurons=16, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7391\n",
      "Testing: layers=2, neurons=16, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7391\n",
      "Testing: layers=2, neurons=16, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=2, neurons=16, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=2, neurons=32, dropout=0.2, lr=0.01\n",
      "  Validation Accuracy: 0.7391\n",
      "Testing: layers=2, neurons=32, dropout=0.2, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "Testing: layers=2, neurons=32, dropout=0.3, lr=0.01\n",
      "  Validation Accuracy: 0.7391\n",
      "Testing: layers=2, neurons=32, dropout=0.3, lr=0.001\n",
      "  Validation Accuracy: 0.7304\n",
      "\n",
      "Best parameters found:\n",
      "  hidden_layers: 1\n",
      "  neurons: 32\n",
      "  dropout_rate: 0.3\n",
      "  learning_rate: 0.001\n",
      "Best validation accuracy: 0.7739\n",
      "\n",
      "Training Final Model with Best Parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 7ms/step - loss: 0.6653 - accuracy: 0.5978 - val_loss: 0.6306 - val_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6927 - val_loss: 0.5920 - val_accuracy: 0.7043 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7449 - val_loss: 0.5642 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.7505 - val_loss: 0.5417 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7598 - val_loss: 0.5283 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4944 - accuracy: 0.7691 - val_loss: 0.5201 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7635 - val_loss: 0.5175 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4773 - accuracy: 0.7616 - val_loss: 0.5151 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7579 - val_loss: 0.5125 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7691 - val_loss: 0.5126 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.7877 - val_loss: 0.5116 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7691 - val_loss: 0.5104 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4690 - accuracy: 0.7840 - val_loss: 0.5116 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.7672 - val_loss: 0.5144 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7840 - val_loss: 0.5176 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.7914 - val_loss: 0.5124 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7803 - val_loss: 0.5138 - val_accuracy: 0.7043 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.7896 - val_loss: 0.5112 - val_accuracy: 0.7130 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.7877 - val_loss: 0.5114 - val_accuracy: 0.7043 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4434 - accuracy: 0.7914 - val_loss: 0.5118 - val_accuracy: 0.6957 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.7914 - val_loss: 0.5127 - val_accuracy: 0.7043 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7840 - val_loss: 0.5120 - val_accuracy: 0.6957 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL EVALUATION\n",
      "============================================================\n",
      "Testing Phase Details:\n",
      "- The test set (15% of data) evaluates the model's generalization to unseen data.\n",
      "- Metrics like accuracy, precision, recall, F1-score, and ROC-AUC are computed.\n",
      "Suggestions for Testing Improvement:\n",
      "- Use stratified k-fold cross-validation for robust evaluation.\n",
      "- Analyze misclassified samples to understand model weaknesses.\n",
      "- Compute confidence intervals for metrics to assess stability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance Analysis:\n",
      "Saved Final model results to output_20250802_202031\\final_model_results.txt\n",
      "Test Accuracy: 0.7845\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68  7]\n",
      " [18 23]]\n",
      "Precision: 0.7667\n",
      "Recall: 0.5610\n",
      "F1-Score: 0.6479\n",
      "ROC-AUC Score: 0.8663\n",
      "\n",
      "ROC Curve Interpretation:\n",
      "The ROC curve shows the trade-off between sensitivity (TPR) and specificity (1-FPR).\n",
      "An AUC of 0.8663 indicates the model's ability to distinguish between classes.\n",
      "Feature Importance (by permutation):\n",
      "  Glucose: 0.1236\n",
      "  BMI: 0.0391\n",
      "  Age: 0.0282\n",
      "  Insulin: 0.0264\n",
      "  SkinThickness: 0.0172\n",
      "  BloodPressure: 0.0052\n",
      "  DiabetesPedigreeFunction: 0.0046\n",
      "  Pregnancies: -0.0017\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "Final Model Architecture:\n",
      "  - Hidden Layers: 1\n",
      "  - Neurons per Layer: 32\n",
      "  - Dropout Rate: 0.3\n",
      "  - Learning Rate: 0.001\n",
      "  - Activation: ReLU (hidden), Sigmoid (output)\n",
      "  - Optimizer: Adam\n",
      "\n",
      "Performance Metrics:\n",
      "  - Test Accuracy: 0.7845\n",
      "  - Precision: 0.7667\n",
      "  - Recall: 0.5610\n",
      "  - F1-Score: 0.6479\n",
      "  - ROC-AUC Score: 0.8663\n",
      "\n",
      "Overfitting Analysis:\n",
      "  - Training Accuracy: 0.7914\n",
      "  - Validation Accuracy: 0.7304\n",
      "  - Accuracy Gap: 0.0610\n",
      "  - Training Loss: 0.4421\n",
      "  - Validation Loss: 0.5104\n",
      "  - Loss Gap: 0.0683\n",
      "  - Mild overfitting, but acceptable\n",
      "\n",
      "Top Contributing Features:\n",
      "  - Glucose: 0.1236\n",
      "  - BMI: 0.0391\n",
      "  - Age: 0.0282\n",
      "\n",
      "Possible Improvements:\n",
      "  - Collect more training data\n",
      "  - Feature engineering (polynomial features, interactions)\n",
      "  - Try different architectures (deeper networks, different activations)\n",
      "  - Ensemble methods\n",
      "  - Advanced regularization techniques\n",
      "C:\\Users\\prade\\miniconda3\\envs\\ml-env2\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "Model saved as output_20250802_202031\\diabetes_model.h5\n",
      "\n",
      "============================================================\n",
      "MODEL DEVELOPMENT COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pkg_resources\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configuration Section\n",
    "CONFIG = {\n",
    "    'RANDOM_STATE': 42,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'MAX_EPOCHS': 100,\n",
    "    'N_PERMUTATIONS': 15,\n",
    "    'TEST_SIZE': 0.3,\n",
    "    'VAL_SIZE': 0.5,\n",
    "    'HIDDEN_LAYERS': [1, 2],\n",
    "    'NEURONS': [ 8 , 16 , 32],\n",
    "    'DROPOUT_RATE': [0.2, 0.3],\n",
    "    'LEARNING_RATE': [0.01, 0.001]\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['RANDOM_STATE'])\n",
    "tf.random.set_seed(CONFIG['RANDOM_STATE'])\n",
    "\n",
    "# Configure logging\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"output_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "log_file = os.path.join(output_dir, 'training_log.log')\n",
    "\n",
    "# Clear existing logging handlers before setting new ones\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(f\"Created output directory: {output_dir}\")\n",
    "logger.info(f\"Logging to file: {log_file}\")\n",
    "\n",
    "# Function to save results to a file in the output directory\n",
    "def save_results_to_file(phase, history, y_test, y_pred, y_pred_proba, feature_importance, X, output_dir):\n",
    "    \"\"\"\n",
    "    Save training results to a file in the output directory.\n",
    "    \n",
    "    Args:\n",
    "        phase (str): Training phase ('Initial' or 'Final').\n",
    "        history: Training history from model.fit().\n",
    "        y_test: True labels for test set.\n",
    "        y_pred: Predicted labels for test set.\n",
    "        y_pred_proba: Predicted probabilities for test set.\n",
    "        feature_importance: List of (feature, importance) tuples.\n",
    "        X: Feature DataFrame for column names.\n",
    "        output_dir (str): Directory to save the results file.\n",
    "    \"\"\"\n",
    "    filename = os.path.join(output_dir, f\"{phase.lower()}_model_results.txt\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    train_acc = max(history.history['accuracy'])\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    train_loss = min(history.history['loss'])\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    acc_gap = train_acc - val_acc\n",
    "    loss_gap = val_loss - train_loss\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"===== {phase} Model Results =====\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\\n\")\n",
    "        f.write(\"Performance Metrics:\\n\")\n",
    "        f.write(f\"  Test Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"  F1-Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"  ROC-AUC Score: {roc_auc:.4f}\\n\")\n",
    "        f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
    "        f.write(\"\\nROC Curve Interpretation:\\n\")\n",
    "        f.write(\"The ROC curve shows the trade-off between sensitivity (TPR) and specificity (1-FPR).\\n\")\n",
    "        f.write(f\"An AUC of {roc_auc:.4f} indicates the model's ability to distinguish between classes.\\n\")\n",
    "        f.write(\"\\nFeature Importance (by permutation):\\n\")\n",
    "        for feature, importance in feature_importance:\n",
    "            f.write(f\"  {feature}: {importance:.4f}\\n\")\n",
    "        f.write(\"\\nOverfitting Analysis:\\n\")\n",
    "        f.write(f\"  Training Accuracy: {train_acc:.4f}\\n\")\n",
    "        f.write(f\"  Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy Gap: {acc_gap:.4f}\\n\")\n",
    "        f.write(f\"  Training Loss: {train_loss:.4f}\\n\")\n",
    "        f.write(f\"  Validation Loss: {val_loss:.4f}\\n\")\n",
    "        f.write(f\"  Loss Gap: {loss_gap:.4f}\\n\")\n",
    "        if acc_gap > 0.1 or loss_gap > train_loss * 0.2:\n",
    "            f.write(\"  Signs of overfitting detected\\n\")\n",
    "            f.write(\"  Recommendations: Increase dropout, reduce model complexity, or collect more data\\n\")\n",
    "        elif acc_gap < 0.02 and abs(loss_gap) < train_loss * 0.1:\n",
    "            f.write(\"  Model appears well-balanced\\n\")\n",
    "        else:\n",
    "            f.write(\"  Mild overfitting, but acceptable\\n\")\n",
    "    \n",
    "    logger.info(f\"Saved {phase} model results to {filename}\")\n",
    "\n",
    "# Check package versions\n",
    "required_versions = {\n",
    "    'numpy': '1.21.0',\n",
    "    'pandas': '1.3.0',\n",
    "    'matplotlib': '3.4.0',\n",
    "    'seaborn': '0.11.0',\n",
    "    'scikit-learn': '1.2.2',\n",
    "    'tensorflow': '2.15.0'\n",
    "}\n",
    "for pkg, min_version in required_versions.items():\n",
    "    try:\n",
    "        installed_version = pkg_resources.get_distribution(pkg).version\n",
    "        logger.info(f\"{pkg} version: {installed_version}\")\n",
    "        if pkg_resources.parse_version(installed_version) < pkg_resources.parse_version(min_version):\n",
    "            logger.warning(f\"{pkg} version {installed_version} is below recommended {min_version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        logger.error(f\"{pkg} is not installed\")\n",
    "        raise ImportError(f\"Please install {pkg}>={min_version}\")\n",
    "\n",
    "# Check GPU availability\n",
    "logger.info(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Report Section: Problem Definition\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"DIABETES PREDICTION USING NEURAL NETWORKS\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"Problem Statement:\")\n",
    "logger.info(\"Diabetes is a chronic condition affecting millions worldwide.\")\n",
    "logger.info(\"Early prediction enables better management and prevents complications.\")\n",
    "logger.info(\"This project builds a neural network to predict diabetes using the Pima Indian Diabetes Dataset.\")\n",
    "\n",
    "# Report Section: Data Preparation\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"DATA PREPARATION\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "try:\n",
    "    df = pd.read_csv(url, names=columns)\n",
    "    logger.info(\"Dataset loaded successfully from URL!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading dataset: {e}\")\n",
    "    raise FileNotFoundError(\"Please ensure internet connection or provide local 'diabetes.csv'.\")\n",
    "\n",
    "# Dataset exploration\n",
    "logger.info(f\"\\nDataset Shape: {df.shape}\")\n",
    "logger.info(f\"Features: {df.columns.tolist()}\")\n",
    "logger.info(\"\\nDataset Overview:\")\n",
    "logger.info(df.info())\n",
    "logger.info(\"\\nStatistical Summary:\")\n",
    "logger.info(df.describe())\n",
    "\n",
    "# Target distribution\n",
    "logger.info(\"\\nTarget Variable Distribution:\")\n",
    "target_counts = df['Outcome'].value_counts()\n",
    "logger.info(f\"Non-diabetic (0): {target_counts[0]} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "logger.info(f\"Diabetic (1): {target_counts[1]} ({target_counts[1]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for missing/zero values\n",
    "logger.info(\"\\nMissing Values:\")\n",
    "logger.info(df.isnull().sum())\n",
    "logger.info(\"\\nZero Values Analysis:\")\n",
    "zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for col in zero_cols:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    logger.info(f\"{col}: {zero_count} zeros ({zero_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Handle zero values with median imputation (robust to outliers)\n",
    "logger.info(\"\\nHandling Missing/Zero Values:\")\n",
    "for col in zero_cols:\n",
    "    if (df[col] == 0).sum() > 0:\n",
    "        median_val = df[df[col] != 0][col].median()\n",
    "        df[col] = df[col].replace(0, median_val)\n",
    "        logger.info(f\"Replaced {col} zeros with median: {median_val:.2f}\")\n",
    "\n",
    "# Data Visualization\n",
    "# Target distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Outcome', data=df, palette=['skyblue', 'salmon'])\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-diabetic', 'Diabetic'])\n",
    "plt.savefig(os.path.join(output_dir, 'target_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "# Feature distributions\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(2, 4, i)\n",
    "    sns.histplot(df[feature], bins=20, kde=True, color='lightblue')\n",
    "    plt.title(f'{feature} Distribution')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_distributions.png'))\n",
    "plt.close()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.savefig(os.path.join(output_dir, 'correlation_heatmap.png'))\n",
    "plt.close()\n",
    "\n",
    "# Feature scaling and data splitting\n",
    "logger.info(\"\\nData Splitting and Scaling:\")\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=CONFIG['TEST_SIZE'], random_state=CONFIG['RANDOM_STATE'], stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=CONFIG['VAL_SIZE'], random_state=CONFIG['RANDOM_STATE'], stratify=y_temp)\n",
    "logger.info(f\"Training set: {X_train.shape[0]} samples (~70%)\")\n",
    "logger.info(f\"Validation set: {X_val.shape[0]} samples (~15%)\")\n",
    "logger.info(f\"Test set: {X_test.shape[0]} samples (~15%)\")\n",
    "logger.info(\"Training set distribution:\")\n",
    "logger.info(pd.Series(y_train).value_counts(normalize=True))\n",
    "logger.info(\"Validation set distribution:\")\n",
    "logger.info(pd.Series(y_val).value_counts(normalize=True))\n",
    "logger.info(\"Test set distribution:\")\n",
    "logger.info(pd.Series(y_test).value_counts(normalize=True))\n",
    "logger.info(\"Note: Stratified splitting ensures class distribution is maintained across all sets.\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "logger.info(\"Feature scaling completed using StandardScaler\")\n",
    "logger.info(\"Scaling ensures features have mean=0 and variance=1, improving model convergence.\")\n",
    "\n",
    "# Report Section: Model Development\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"MODEL DEVELOPMENT\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"Training Phase Details:\")\n",
    "logger.info(\"- The model is trained on 70% of the data to learn patterns.\")\n",
    "logger.info(\"- Early stopping monitors validation loss to prevent overfitting.\")\n",
    "logger.info(\"- Learning rate reduction adjusts the learning rate if validation loss plateaus.\")\n",
    "logger.info(\"Suggestions for Training Improvement:\")\n",
    "logger.info(\"- Use k-fold cross-validation to reduce variance in performance estimates.\")\n",
    "logger.info(\"- Implement data augmentation techniques for small datasets.\")\n",
    "logger.info(\"- Use class weights to handle imbalanced data.\")\n",
    "logger.info(\"- Explore batch normalization to stabilize training.\")\n",
    "\n",
    "def create_model(hidden_layers=1, neurons=16, dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Create a feedforward neural network for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        hidden_layers (int): Number of hidden layers.\n",
    "        neurons (int): Number of neurons per hidden layer.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        learning_rate (float): Learning rate for Adam optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        Sequential: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initial model training\n",
    "logger.info(\"\\nBuilding Initial Model:\")\n",
    "try:\n",
    "    model = create_model()\n",
    "    logger.info(model.summary())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error building model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Train with early stopping and learning rate reduction\n",
    "try:\n",
    "    history = model.fit(X_train_scaled, y_train,\n",
    "                       batch_size=CONFIG['BATCH_SIZE'],\n",
    "                       epochs=CONFIG['MAX_EPOCHS'],\n",
    "                       validation_data=(X_val_scaled, y_val),\n",
    "                       verbose=1,\n",
    "                       callbacks=[\n",
    "                           tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "                           tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss')\n",
    "                       ])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training initial model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save initial model results\n",
    "try:\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    # Compute feature importance for initial model\n",
    "    initial_feature_importance = []\n",
    "    baseline_score = model.evaluate(X_test_scaled, y_test, verbose=0)[1]\n",
    "    for i, feature in enumerate(X.columns):\n",
    "        importance_scores = []\n",
    "        for _ in range(CONFIG['N_PERMUTATIONS']):\n",
    "            X_test_permuted = X_test_scaled.copy()\n",
    "            X_test_permuted[:, i] = np.random.permutation(X_test_permuted[:, i])\n",
    "            permuted_score = model.evaluate(X_test_permuted, y_test, verbose=0)[1]\n",
    "            importance_scores.append(baseline_score - permuted_score)\n",
    "        initial_feature_importance.append((feature, np.mean(importance_scores)))\n",
    "    initial_feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    save_results_to_file(\"Initial\", history, y_test, y_pred, y_pred_proba, initial_feature_importance, X, output_dir)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error evaluating initial model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "plt.close()\n",
    "\n",
    "# Report Section: Hyperparameter Tuning\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"HYPERPARAMETER TUNING\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"Hyperparameter tuning uses a grid search over the configuration parameters.\")\n",
    "logger.info(\"Validation set performance guides the selection of the best model parameters.\")\n",
    "logger.info(\"Suggestions for Tuning Improvement:\")\n",
    "logger.info(\"- Use random search or Bayesian optimization for efficiency.\")\n",
    "logger.info(\"- Expand the parameter grid for finer granularity.\")\n",
    "logger.info(\"- Incorporate learning rate schedules for adaptive learning.\")\n",
    "\n",
    "# Manual hyperparameter tuning\n",
    "param_grid = {\n",
    "    'hidden_layers': CONFIG['HIDDEN_LAYERS'],\n",
    "    'neurons': CONFIG['NEURONS'],\n",
    "    'dropout_rate': CONFIG['DROPOUT_RATE'],\n",
    "    'learning_rate': CONFIG['LEARNING_RATE']\n",
    "}\n",
    "\n",
    "logger.info(\"Parameter grid for Manual Search:\")\n",
    "for param, values in param_grid.items():\n",
    "    logger.info(f\"  {param}: {values}\")\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "results = []\n",
    "\n",
    "logger.info(\"\\nStarting hyperparameter tuning...\")\n",
    "for hidden_layers in param_grid['hidden_layers']:\n",
    "    for neurons in param_grid['neurons']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                try:\n",
    "                    logger.info(f\"Testing: layers={hidden_layers}, neurons={neurons}, \"\n",
    "                                f\"dropout={dropout_rate}, lr={learning_rate}\")\n",
    "                    model_temp = create_model(hidden_layers, neurons, dropout_rate, learning_rate)\n",
    "                    history_temp = model_temp.fit(X_train_scaled, y_train,\n",
    "                                                 batch_size=CONFIG['BATCH_SIZE'],\n",
    "                                                 epochs=CONFIG['MAX_EPOCHS'],\n",
    "                                                 validation_data=(X_val_scaled, y_val),\n",
    "                                                 verbose=0,\n",
    "                                                 callbacks=[\n",
    "                                                     tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
    "                                                 ])\n",
    "                    val_score = max(history_temp.history['val_accuracy'])\n",
    "                    results.append({\n",
    "                        'hidden_layers': hidden_layers,\n",
    "                        'neurons': neurons,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'val_accuracy': val_score\n",
    "                    })\n",
    "                    if val_score > best_score:\n",
    "                        best_score = val_score\n",
    "                        best_params = {\n",
    "                            'hidden_layers': hidden_layers,\n",
    "                            'neurons': neurons,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'learning_rate': learning_rate\n",
    "                        }\n",
    "                    logger.info(f\"  Validation Accuracy: {val_score:.4f}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in hyperparameter tuning: {e}\")\n",
    "                    continue\n",
    "\n",
    "logger.info(f\"\\nBest parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    logger.info(f\"  {param}: {value}\")\n",
    "logger.info(f\"Best validation accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "logger.info(\"\\nTraining Final Model with Best Parameters:\")\n",
    "try:\n",
    "    final_model = create_model(**best_params)\n",
    "    final_history = final_model.fit(X_train_scaled, y_train,\n",
    "                                   batch_size=CONFIG['BATCH_SIZE'],\n",
    "                                   epochs=CONFIG['MAX_EPOCHS'],\n",
    "                                   validation_data=(X_val_scaled, y_val),\n",
    "                                   verbose=1,\n",
    "                                   callbacks=[\n",
    "                                       tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "                                       tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss')\n",
    "                                   ])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training final model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Report Section: Model Evaluation\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"MODEL EVALUATION\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"Testing Phase Details:\")\n",
    "logger.info(\"- The test set (15% of data) evaluates the model's generalization to unseen data.\")\n",
    "logger.info(\"- Metrics like accuracy, precision, recall, F1-score, and ROC-AUC are computed.\")\n",
    "logger.info(\"Suggestions for Testing Improvement:\")\n",
    "logger.info(\"- Use stratified k-fold cross-validation for robust evaluation.\")\n",
    "logger.info(\"- Analyze misclassified samples to understand model weaknesses.\")\n",
    "logger.info(\"- Compute confidence intervals for metrics to assess stability.\")\n",
    "\n",
    "# Predictions and feature importance for final model\n",
    "try:\n",
    "    y_pred_proba = final_model.predict(X_test_scaled)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Feature Importance\n",
    "    logger.info(\"\\nFeature Importance Analysis:\")\n",
    "    feature_importance = []\n",
    "    baseline_score = final_model.evaluate(X_test_scaled, y_test, verbose=0)[1]\n",
    "    for i, feature in enumerate(X.columns):\n",
    "        importance_scores = []\n",
    "        for _ in range(CONFIG['N_PERMUTATIONS']):\n",
    "            X_test_permuted = X_test_scaled.copy()\n",
    "            X_test_permuted[:, i] = np.random.permutation(X_test_permuted[:, i])\n",
    "            permuted_score = final_model.evaluate(X_test_permuted, y_test, verbose=0)[1]\n",
    "            importance_scores.append(baseline_score - permuted_score)\n",
    "        feature_importance.append((feature, np.mean(importance_scores)))\n",
    "    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # Save final model results\n",
    "    save_results_to_file(\"Final\", final_history, y_test, y_pred, y_pred_proba, feature_importance, X, output_dir)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error evaluating final model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "logger.info(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "logger.info(f\"\\nConfusion Matrix:\\n{conf_matrix}\")\n",
    "logger.info(f\"Precision: {precision:.4f}\")\n",
    "logger.info(f\"Recall: {recall:.4f}\")\n",
    "logger.info(f\"F1-Score: {f1:.4f}\")\n",
    "logger.info(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "logger.info(\"\\nROC Curve Interpretation:\")\n",
    "logger.info(\"The ROC curve shows the trade-off between sensitivity (TPR) and specificity (1-FPR).\")\n",
    "logger.info(f\"An AUC of {roc_auc:.4f} indicates the model's ability to distinguish between classes.\")\n",
    "\n",
    "logger.info(\"Feature Importance (by permutation):\")\n",
    "for feature, importance in feature_importance:\n",
    "    logger.info(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "# Save evaluation plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Non-diabetic', 'Diabetic'],\n",
    "            yticklabels=['Non-diabetic', 'Diabetic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'evaluation_plots.png'))\n",
    "plt.close()\n",
    "\n",
    "# Report Section: Results and Discussion\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"RESULTS SUMMARY\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "logger.info(f\"Final Model Architecture:\")\n",
    "logger.info(f\"  - Hidden Layers: {best_params['hidden_layers']}\")\n",
    "logger.info(f\"  - Neurons per Layer: {best_params['neurons']}\")\n",
    "logger.info(f\"  - Dropout Rate: {best_params['dropout_rate']}\")\n",
    "logger.info(f\"  - Learning Rate: {best_params['learning_rate']}\")\n",
    "logger.info(f\"  - Activation: ReLU (hidden), Sigmoid (output)\")\n",
    "logger.info(f\"  - Optimizer: Adam\")\n",
    "\n",
    "logger.info(f\"\\nPerformance Metrics:\")\n",
    "logger.info(f\"  - Test Accuracy: {accuracy:.4f}\")\n",
    "logger.info(f\"  - Precision: {precision:.4f}\")\n",
    "logger.info(f\"  - Recall: {recall:.4f}\")\n",
    "logger.info(f\"  - F1-Score: {f1:.4f}\")\n",
    "logger.info(f\"  - ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Overfitting analysis\n",
    "train_acc = max(final_history.history['accuracy'])\n",
    "val_acc = max(final_history.history['val_accuracy'])\n",
    "train_loss = min(final_history.history['loss'])\n",
    "val_loss = min(final_history.history['val_loss'])\n",
    "acc_gap = train_acc - val_acc\n",
    "loss_gap = val_loss - train_loss\n",
    "\n",
    "logger.info(\"\\nOverfitting Analysis:\")\n",
    "logger.info(f\"  - Training Accuracy: {train_acc:.4f}\")\n",
    "logger.info(f\"  - Validation Accuracy: {val_acc:.4f}\")\n",
    "logger.info(f\"  - Accuracy Gap: {acc_gap:.4f}\")\n",
    "logger.info(f\"  - Training Loss: {train_loss:.4f}\")\n",
    "logger.info(f\"  - Validation Loss: {val_loss:.4f}\")\n",
    "logger.info(f\"  - Loss Gap: {loss_gap:.4f}\")\n",
    "if acc_gap > 0.1 or loss_gap > train_loss * 0.2:\n",
    "    logger.info(\"  - Signs of overfitting detected\")\n",
    "    logger.info(\"  - Recommendations: Increase dropout, reduce model complexity, or collect more data\")\n",
    "elif acc_gap < 0.02 and abs(loss_gap) < train_loss * 0.1:\n",
    "    logger.info(\"  - Model appears well-balanced\")\n",
    "else:\n",
    "    logger.info(\"  - Mild overfitting, but acceptable\")\n",
    "\n",
    "logger.info(f\"\\nTop Contributing Features:\")\n",
    "for feature, importance in feature_importance[:3]:\n",
    "    logger.info(f\"  - {feature}: {importance:.4f}\")\n",
    "\n",
    "logger.info(f\"\\nPossible Improvements:\")\n",
    "logger.info(\"  - Collect more training data\")\n",
    "logger.info(\"  - Feature engineering (polynomial features, interactions)\")\n",
    "logger.info(\"  - Try different architectures (deeper networks, different activations)\")\n",
    "logger.info(\"  - Ensemble methods\")\n",
    "logger.info(\"  - Advanced regularization techniques\")\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    final_model.save(os.path.join(output_dir, 'diabetes_model.h5'))\n",
    "    logger.info(f\"Model saved as {os.path.join(output_dir, 'diabetes_model.h5')}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving model: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"MODEL DEVELOPMENT COMPLETED\")\n",
    "logger.info(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32896c42-60c8-487d-b529-39f42686af99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
